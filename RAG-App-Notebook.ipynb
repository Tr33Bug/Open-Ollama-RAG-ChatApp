{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-Ollama-RAG-ChatApp Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain\n",
    "# %pip install unstructured\n",
    "# %pip install chromadb\n",
    "# %pip install langchain_community\n",
    "# %pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial database?\n",
    "# -> Set to True if you run the notebook for the first time or if you changed the md files\n",
    "initial_db = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/\"\n",
    "OLLAMA_MODEL = \"llama2:chat\"\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "CHROMA_PATH = \"chroma/\"\n",
    "\n",
    "## langchain split config\n",
    "# md headers\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "# chunk sizes\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chunks from md files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    loader = TextLoader(DATA_PATH + file)\n",
    "    documents.append(loader.load()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[15].metadata\n",
    "len(documents[15].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in documents:\n",
    "#     print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "chunks_array= []\n",
    "\n",
    "\n",
    "for doc in documents:\n",
    "    chunks_array.append(text_splitter.split_text(doc.page_content))\n",
    "    # append source metadata to each chunk\n",
    "    for chunk in chunks_array[-1]:\n",
    "        # combine metadate\n",
    "        chunk.metadata = doc.metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char-level splits\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len, add_start_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_array_txt_base = []\n",
    "counter = 0\n",
    "for document in chunks_array:\n",
    "    for chunk in document:\n",
    "        splits = text_splitter.split_documents([chunk])\n",
    "        chunks_array_txt_base.append(splits)\n",
    "        counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(counter)\n",
    "len(chunks_array_txt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_document_chunks = [chunk for document in chunks_array_txt_base for chunk in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n"
     ]
    }
   ],
   "source": [
    "print(len(all_document_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Thema  \\n### Das Thema oder die Frage in einem Satz:\\n> Was braucht man f√ºr Hardware und was kostet es wenn man sein eigenes LLAMA zuhause haben m√∂chte.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_document_chunks[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to ollama backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start ollama\n",
    "# import subprocess\n",
    "\n",
    "# subprocess.Popen(\"ollama serve\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command = f\"ollama pull {OLLAMA_MODEL}\"\n",
    "\n",
    "# process = subprocess.Popen(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tree/Documents/Dev/RAG/RAG/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamagoyaki is a popular Japanese dish that consists of rolled omelette filled with sweet bean paste or other ingredients. The word \"tamago\" means \"egg,\" and \"yaki\" means \"grilled\" or \"cooked.\" Tamagoyaki is often served as a snack or dessert in Japan, and it is known for its smooth, creamy texture and sweet flavor.\n",
      "\n",
      "The dish typically consists of a thin layer of beaten eggs that are rolled into a cylindrical shape and filled with sweet bean paste, such as azuki bean paste or anko. The eggs are then rolled up again and cooked in a special pan called a tamagoyaki pan, which has a non-stick surface and is designed to create the characteristic spiral pattern of the dish.\n",
      "\n",
      "Tamagoyaki can be served on its own or with other Japanese sweets, such as mochi or manju. It is also sometimes used as a filling for other desserts, such as ice cream or cake. Overall, tamagoyaki is a popular and delicious treat in Japan that is enjoyed by people of all ages.\n"
     ]
    }
   ],
   "source": [
    "# TEST OLLAMA CONNECTION ##\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "ollama = Ollama(base_url=OLLAMA_URL, model=OLLAMA_MODEL)\n",
    "\n",
    "print(ollama(\"what ist Tamagoyaki?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='---\\ntags:\\n- research\\n- master-thesis\\naliases:\\n- Rechenleistung f√ºr Lokale LLMs\\nstatus:\\ntype: research-note\\nsubject: ml\\ntopic: \"[[020_Topic_Master-Thesis]]\"\\n---', metadata={'source': 'data/Rechenleistung f√ºr Lokale LLMs.md', 'start_index': 0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_document_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chroma db or load db from disk\n",
    "if initial_db:\n",
    "    from langchain.embeddings import OllamaEmbeddings\n",
    "    from langchain.vectorstores import Chroma\n",
    "\n",
    "    chroma_db = Chroma.from_documents(all_document_chunks, OllamaEmbeddings(base_url=OLLAMA_URL, model=OLLAMA_MODEL), persist_directory=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load chroma db from disk\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OllamaEmbeddings(base_url=OLLAMA_URL, model=OLLAMA_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test similarity search\n",
    "query = \"How to make Japanese Egg Rolls?\"\n",
    "\n",
    "result_docs = chroma_db.similarity_search(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='8. Serve the Panang Curry with a bowl of steamed rice and garnish it with additional Thai basil leaves and roasted peanuts.' metadata={'source': 'data/Thai Panag Curry from Reddit üçõ.md', 'start_index': 817}\n",
      "page_content='### FX\\nRiser/Crashes (Position, Style):\\nEar-Candy(Position, Description):' metadata={'source': 'data/Track-Analyse - Top Down Ansatz.md', 'start_index': 0}\n",
      "page_content='3. Add the water and coconut milk to the pan and bring it to a boil.\\n4. Add the green beans to the pan and let it simmer for 10 minutes.\\n5. Meanwhile, cut the tofu into cubes and fry them in oil until golden on both sides.\\n6. Add the bell peppers, fried tofu, and Thai basil leaves to the pan and simmer for an additional 5 minutes.\\n7. Season the curry with salt, sugar, and soy sauce according to your taste. Stir in the Thai basil leaves.' metadata={'source': 'data/Thai Panag Curry from Reddit üçõ.md', 'start_index': 376}\n",
      "page_content='x---\\ntags:\\n- music-production\\naliases:\\nimg_link:\\nstatus: idee\\ntype: guide\\ntopic: \"[[040_Topic_Music-Production]]\"\\nsubject: music-production\\n---\\nMastering Chain in Ableton from Reddit  \\n**Related Links:**\\n- [[Guide - Mixing]]\\n- [[Guide - Mastering]]' metadata={'source': 'data/Reddit Mastering Chain.md', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "for doc in result_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "\n",
    "\n",
    "def chat_ollama(message, history):\n",
    "    # initiate ollama\n",
    "    ollama = Ollama(base_url=OLLAMA_URL, model=OLLAMA_MODEL)\n",
    "\n",
    "    # search for similar documents in chroma db\n",
    "    result_chunks = chroma_db.similarity_search(message)\n",
    "    \n",
    "    chroma_knowledge = \"\"\n",
    "    for id, chunk in enumerate(result_chunks):\n",
    "        source_id = id + 1\n",
    "        chroma_knowledge += \"[\" + str(source_id) +\"] \\n\" + chunk.page_content + \"\\n\"\n",
    "\n",
    "    sources = \"\"\n",
    "    for id, chunk in enumerate(result_chunks):\n",
    "        source_id = id + 1\n",
    "        sources += \"[\" + str(source_id) + \"] \\n\" + chunk.metadata[\"source\"] + \"\\n\"\n",
    "\n",
    "    prompt = \"Answer the following question using the provided knowledge and the chat history:\\n\\n###KNOWLEDGE: \" + chroma_knowledge + \"\\n###CHAT-HISTORY: \" + str(history) + \"\\n\\n###QUESTION: \" + message\n",
    "    result = ollama(prompt) + \"\\n\\n\\nReferences:\\n\" + sources \n",
    "\n",
    "    # print(prompt)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Great! Based on the provided knowledge and chat history, here's how to make Japanese Egg Rolls:\\n\\nIngredients:\\n\\n* 2 pieces garlic\\n* 1 small piece ginger\\n* Drizzle of olive oil\\n* Handful of shimeji mushrooms\\n* 1 cup extra firm tofu\\n* 2 tbsp peanut butter\\n* 2 cups vegetable stock (https://www.youtube.com/watch?v=AgvFmD6sz9I&t=0s)\\n* 1 portion ramen noodles (https://www.youtube.com/watch?v=vPvLw5kDpm8&t=0s)\\n* 1 tbsp cane sugar\\n* 1 tsp rice vinegar\\n* 2 tbsp soy sauce\\n* 1 tsp sesame oil\\n* 2 tbsp roasted peanuts\\n* 1 stick green onion\\n* Few sprigs cilantro\\n\\nInstructions:\\n\\n1. Heat the olive oil in a pan over medium heat. Add the garlic and ginger and saut√© until fragrant.\\n2. Add the mushrooms and tofu and stir-fry until the tofu is golden brown.\\n3. Add the peanut butter and vegetable stock and stir-fry until the mixture thickens.\\n4. Add the ramen noodles and stir-fry until they are cooked through.\\n5. Season with soy sauce, rice vinegar, sesame oil, cane sugar, and cilantro.\\n6. Serve the Japanese Egg Rolls hot with a side of roasted peanuts.\\n\\nI hope this helps! Let me know if you have any questions or need further clarification.\\n\\n\\nReferences:\\n[1] \\ndata/Vegan Peanut Satay Ramen üçú.md\\n[2] \\ndata/Thai Panag Curry from Reddit üçõ.md\\n[3] \\ndata/Miso Soup with Rice Noodles üç≤.md\\n[4] \\ndata/Vegan Peanut Satay Ramen üçú.md\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_ollama(\"How to make Japanese Eggrolls\", \"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "gradio_interface = gr.ChatInterface(\n",
    "        chat_ollama,\n",
    "        chatbot=gr.Chatbot(),\n",
    "        textbox=gr.Textbox(placeholder=\"Example: How to make Japanese Egg Rolls?\", container=False, scale=7),\n",
    "        title=\"The Ollama test chatbot\",\n",
    "        description=f\"Ask the {OLLAMA_MODEL} chatbot a question!\",\n",
    "        theme='gradio/base', # themes at https://huggingface.co/spaces/gradio/theme-gallery\n",
    "        retry_btn=None,\n",
    "        undo_btn=\"Delete Previous\",\n",
    "        clear_btn=\"Clear\",\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradio_interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END OF FILE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
